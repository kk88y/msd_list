--- a/mac80211.c
+++ b/mac80211.c
@@ -921,8 +921,7 @@
 {
 	struct ieee80211_sta *sta;
 	struct ieee80211_hw *hw;
-	struct sk_buff *skb, *tmp;
-	LIST_HEAD(list);
+	struct sk_buff *skb;
 
 	spin_lock(&dev->rx_lock);
 	while ((skb = __skb_dequeue(frames)) != NULL) {
@@ -1011,7 +1010,7 @@
 
 		skb_shinfo(skb)->frag_list = NULL;
 		mt76_rx_convert(dev, skb, &hw, &sta);
-		ieee80211_rx_list(hw, sta, skb, &list);
+		ieee80211_rx_napi(hw, sta, skb, napi);
 
 		/* subsequent amsdu frames */
 		while (nskb) {
@@ -1020,20 +1019,10 @@
 			skb->next = NULL;
 
 			mt76_rx_convert(dev, skb, &hw, &sta);
-			ieee80211_rx_list(hw, sta, skb, &list);
+			ieee80211_rx_napi(hw, sta, skb, napi);
 		}
 	}
 	spin_unlock(&dev->rx_lock);
-
-	if (!napi) {
-		netif_receive_skb_list(&list);
-		return;
-	}
-
-	list_for_each_entry_safe(skb, tmp, &list, list) {
-		skb_list_del_init(skb);
-		napi_gro_receive(napi, skb);
-	}
 }
 
 void mt76_rx_poll_complete(struct mt76_dev *dev, enum mt76_rxq_id q,
--- a/mt76.h
+++ b/mt76.h
@@ -1056,13 +1056,7 @@
 				       struct sk_buff_head *list);
 void mt76_tx_status_skb_done(struct mt76_dev *dev, struct sk_buff *skb,
 			     struct sk_buff_head *list);
-void __mt76_tx_complete_skb(struct mt76_dev *dev, u16 wcid, struct sk_buff *skb,
-			    struct list_head *free_list);
-static inline void
-mt76_tx_complete_skb(struct mt76_dev *dev, u16 wcid, struct sk_buff *skb)
-{
-    __mt76_tx_complete_skb(dev, wcid, skb, NULL);
-}
+void mt76_tx_complete_skb(struct mt76_dev *dev, u16 wcid, struct sk_buff *skb);
 
 void mt76_tx_status_check(struct mt76_dev *dev, struct mt76_wcid *wcid,
 			  bool flush);
--- a/tx.c
+++ b/tx.c
@@ -54,27 +54,10 @@
 
 	spin_unlock_bh(&dev->status_list.lock);
 
-	rcu_read_lock();
 	while ((skb = __skb_dequeue(list)) != NULL) {
-		struct ieee80211_tx_status status = {
-			.skb = skb,
-			.info = IEEE80211_SKB_CB(skb),
-		};
-		struct mt76_tx_cb *cb = mt76_tx_skb_cb(skb);
-		struct mt76_wcid *wcid;
-
-		wcid = rcu_dereference(dev->wcid[cb->wcid]);
-		if (wcid) {
-			status.sta = wcid_to_sta(wcid);
-
-			if (status.sta)
-				status.rate = &wcid->rate;
-		}
-
 		hw = mt76_tx_status_get_hw(dev, skb);
-		ieee80211_tx_status_ext(hw, &status);
+		ieee80211_tx_status(hw, skb);
 	}
-	rcu_read_unlock();
 }
 EXPORT_SYMBOL_GPL(mt76_tx_status_unlock);
 
@@ -92,7 +75,7 @@
 
 	/* Tx status can be unreliable. if it fails, mark the frame as ACKed */
 	if (flags & MT_TX_CB_TXS_FAILED) {
-		info->status.rates[0].count = 0;
+		ieee80211_tx_info_clear_status(info);
 		info->status.rates[0].idx = -1;
 		info->flags |= IEEE80211_TX_STAT_ACK;
 	}
@@ -185,37 +168,36 @@
 EXPORT_SYMBOL_GPL(mt76_tx_status_check);
 
 static void
-mt76_tx_check_non_aql(struct mt76_dev *dev, struct mt76_wcid *wcid,
-		      struct sk_buff *skb)
+mt76_tx_check_non_aql(struct mt76_dev *dev, u16 wcid_idx, struct sk_buff *skb)
 {
 	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);
+	struct mt76_wcid *wcid;
 	int pending;
 
-	if (!wcid || info->tx_time_est)
+	if (info->tx_time_est)
 		return;
 
-	pending = atomic_dec_return(&wcid->non_aql_packets);
-	if (pending < 0)
-		atomic_cmpxchg(&wcid->non_aql_packets, pending, 0);
+	if (wcid_idx >= ARRAY_SIZE(dev->wcid))
+		return;
+
+	rcu_read_lock();
+
+	wcid = rcu_dereference(dev->wcid[wcid_idx]);
+	if (wcid) {
+		pending = atomic_dec_return(&wcid->non_aql_packets);
+		if (pending < 0)
+			atomic_cmpxchg(&wcid->non_aql_packets, pending, 0);
+	}
+
+	rcu_read_unlock();
 }
 
-void __mt76_tx_complete_skb(struct mt76_dev *dev, u16 wcid_idx, struct sk_buff *skb,
-			    struct list_head *free_list)
-{
-	struct ieee80211_tx_status status = {
-		.skb = skb,
-		.free_list = free_list,
-	};
-	struct mt76_wcid *wcid = NULL;
+void mt76_tx_complete_skb(struct mt76_dev *dev, u16 wcid_idx, struct sk_buff *skb)
+{
 	struct ieee80211_hw *hw;
 	struct sk_buff_head list;
 
-	rcu_read_lock();
-
-	if (wcid_idx < ARRAY_SIZE(dev->wcid))
-		wcid = rcu_dereference(dev->wcid[wcid_idx]);
-
-	mt76_tx_check_non_aql(dev, wcid, skb);
+	mt76_tx_check_non_aql(dev, wcid_idx, skb);
 
 #ifdef CONFIG_NL80211_TESTMODE
 	if (mt76_is_testmode_skb(dev, skb, &hw)) {
@@ -227,25 +209,21 @@
 			wake_up(&dev->tx_wait);
 
 		dev_kfree_skb_any(skb);
-		goto out;
+		return;
 	}
 #endif
 
 	if (!skb->prev) {
 		hw = mt76_tx_status_get_hw(dev, skb);
-		status.sta = wcid_to_sta(wcid);
-		ieee80211_tx_status_ext(hw, &status);
-		goto out;
+		ieee80211_free_txskb(hw, skb);
+		return;
 	}
 
 	mt76_tx_status_lock(dev, &list);
 	__mt76_tx_status_skb_done(dev, skb, MT_TX_CB_DMA_DONE, &list);
 	mt76_tx_status_unlock(dev, &list);
-
-out:
-	rcu_read_unlock();
 }
-EXPORT_SYMBOL_GPL(__mt76_tx_complete_skb);
+EXPORT_SYMBOL_GPL(mt76_tx_complete_skb);
 
 static int
 __mt76_tx_queue_skb(struct mt76_phy *phy, int qid, struct sk_buff *skb,
--- a/mt7915/mac.c
+++ b/mt7915/mac.c
@@ -1091,7 +1091,7 @@
 
 static void
 mt7915_txwi_free(struct mt7915_dev *dev, struct mt76_txwi_cache *t,
-		 struct ieee80211_sta *sta, struct list_head *free_list)
+		 struct ieee80211_sta *sta)
 {
 	struct mt76_dev *mdev = &dev->mt76;
 	struct mt76_wcid *wcid;
@@ -1113,7 +1113,7 @@
 		wcid_idx = FIELD_GET(MT_TXD1_WLAN_IDX, le32_to_cpu(txwi[1]));
 	}
 
-	__mt76_tx_complete_skb(mdev, wcid_idx, t->skb, free_list);
+	mt76_tx_complete_skb(mdev, wcid_idx, t->skb);
 
 out:
 	t->skb = NULL;
@@ -1128,8 +1128,6 @@
 	struct mt76_phy *mphy_ext = mdev->phy2;
 	struct mt76_txwi_cache *txwi;
 	struct ieee80211_sta *sta = NULL;
-	LIST_HEAD(free_list);
-	struct sk_buff *tmp;
 	u8 i, count;
 	bool wake = false;
 
@@ -1186,7 +1184,7 @@
 		if (!txwi)
 			continue;
 
-		mt7915_txwi_free(dev, txwi, sta, &free_list);
+		mt7915_txwi_free(dev, txwi, sta);
 	}
 
 	mt7915_mac_sta_poll(dev);
@@ -1195,13 +1193,6 @@
 		mt76_set_tx_blocked(&dev->mt76, false);
 
 	mt76_worker_schedule(&dev->mt76.tx_worker);
-
-	napi_consume_skb(skb, 1);
-
-	list_for_each_entry_safe(skb, tmp, &free_list, list) {
-		skb_list_del_init(skb);
-		napi_consume_skb(skb, 1);
-	}
 }
 
 static bool
@@ -1604,7 +1595,7 @@
 
 	spin_lock_bh(&dev->mt76.token_lock);
 	idr_for_each_entry(&dev->mt76.token, txwi, id) {
-		mt7915_txwi_free(dev, txwi, NULL, NULL);
+		mt7915_txwi_free(dev, txwi, NULL);
 		dev->mt76.token_count--;
 	}
 	spin_unlock_bh(&dev->mt76.token_lock);
--- a/mt7921/mac.c
+++ b/mt7921/mac.c
@@ -935,15 +935,13 @@
 
 static void
 mt7921_tx_complete_status(struct mt76_dev *mdev, struct sk_buff *skb,
-			  struct ieee80211_sta *sta, u8 stat,
-			  struct list_head *free_list)
+			  struct ieee80211_sta *sta, u8 stat)
 {
 	struct ieee80211_tx_info *info = IEEE80211_SKB_CB(skb);
 	struct ieee80211_tx_status status = {
 		.sta = sta,
 		.info = info,
 		.skb = skb,
-		.free_list = free_list,
 	};
 	struct ieee80211_hw *hw;
 
@@ -1006,8 +1004,6 @@
 	struct mt76_dev *mdev = &dev->mt76;
 	struct mt76_txwi_cache *txwi;
 	struct ieee80211_sta *sta = NULL;
-	LIST_HEAD(free_list);
-	struct sk_buff *tmp;
 	bool wake = false;
 	u8 i, count;
 
@@ -1075,7 +1071,7 @@
 					atomic_cmpxchg(&wcid->non_aql_packets, pending, 0);
 			}
 
-			mt7921_tx_complete_status(mdev, txwi->skb, sta, stat, &free_list);
+			mt7921_tx_complete_status(mdev, txwi->skb, sta, stat);
 			txwi->skb = NULL;
 		}
 
@@ -1085,13 +1081,6 @@
 	if (wake)
 		mt76_set_tx_blocked(&dev->mt76, false);
 
-	napi_consume_skb(skb, 1);
-
-	list_for_each_entry_safe(skb, tmp, &free_list, list) {
-		skb_list_del_init(skb);
-		napi_consume_skb(skb, 1);
-	}
-
 	mt7921_mac_sta_poll(dev);
 	mt76_worker_schedule(&dev->mt76.tx_worker);
 }
@@ -1125,8 +1114,7 @@
 
 		wcid = rcu_dereference(dev->mt76.wcid[cb->wcid]);
 
-		mt7921_tx_complete_status(mdev, e->skb, wcid_to_sta(wcid), 0,
-					  NULL);
+		mt7921_tx_complete_status(mdev, e->skb, wcid_to_sta(wcid), 0);
 	}
 }
 
